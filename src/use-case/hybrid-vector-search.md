# Hybrid Vector Search

In the past, hybrid search combined two search methods: traditional keyword-based search and vector-based similarity search. However, sparse vector could act as a substitute for keyword search, unlocking the full potential of the data pipeline with pure vector search.

::: tip
For more information about keyword-based hybrid search, please refer to [Hybrid Search](./hybrid-search.md)
:::


In this post we will explore how [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3) and `pgvecto.rs` can be used to exploit the potential of hybrid search using both sparse and dense vectors.

Let's dive in.

## An Overview of Sparse and Dense Vector Embedding

Sparse vectors are very high-dimensional but contain few non-zero values, making them suitable for traditional information retrieval use cases. Let's say we have a vector with 32000 dimensions, but only two elements of it are non-zero, this is a typical sparse vector.
$$
S[1\times 32000]=\left[
 \begin{matrix}
   0 & 0 & 0.015 & 0 & 0 & 0 & 0 & \cdots & 0.543 & 0
  \end{matrix}
  \right]
$$


Dense vectors, on the other hand, are embeddings from neural networks. These vectors are typically generated by text embedding models and are characterized by most or all elements being non-zero. They typically have fewer dimensions, such as 256 or 1536, much less than sparse vectors.
$$
D[1\times 256]=\left[
 \begin{matrix}
   0.342 & 1.774 & 0.087 & 0.321 & 0.664 & 0.870 & 0.001 & \cdots & 0.543 & 0.999
  \end{matrix}
  \right]
$$


Dense vectors and sparse vectors are both used to represent features of text chunks, but they have significant differences. A single dimension in a dense vector embedding does not mean anything, as it is too abstract to determine its meaning. However, when we take all the dimensions together, they provide the semantic meaning of the input text.

In contrast, sparse vectors are related to the vocabulary, with dimensions equal to the vocabulary size. Therefore, the dimensions can reach tens of thousands or even hundreds of thousands, but most elements are zero. Each element of a sparse vector corresponds to the weight of a word in the text fragment.

In general, words with higher weights indicate a higher importance in the text. It is much easier for users to analyze or visualize the relationship between the text and the vocabulary with sparse vectors. While for dense vectors, some dimension reduction algorithms are required for visualization, such as:

- [Principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)
- [T-distributed stochastic neighbor embedding (T-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
- [Uniform Manifold Approximation and Projection (UMAP)](https://arxiv.org/abs/1802.03426)

|                      | Dense Vector          | Sparse Vector               |
| -------------------- | --------------------- | --------------------------- |
| Dimension            | Hundreds to thousands | Tens of thousands           |
| Dimension Meaning    | Nothing               | Word Weight                 |
| Interpretability     | Hard                  | Easy                        |
| Usage                | Vector Distance       | Vector Distance / Visualize |
| Semantic sensitivity | High                  | Low                         |

Sparse vectors are generally less semantically sensitive than dense vectors. They have a harder time capturing the relationships between words, especially for synonyms and related terms.

For example, the words "rocket" and "SPACE X" are both related to space exploration. Dense vectors are more likely to reveal this similarity and assign a closer vector distance to the corresponding texts of these words.

## How to create a vector embedding?

A vector embedding is the internal representation of input data in deep learning models, also known as embedding models. Most embedding models, such as `text-embedding-3-small`, are dense models, that only output dense embddings. We need special models designed for sparse embedding.

This is where [SPLADE](https://europe.naverlabs.com/research/computer-science/splade-a-sparse-bi-encoder-bert-based-model-achieves-effective-and-efficient-full-text-document-ranking/?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) and [BGE-M3](https://arxiv.org/pdf/2402.03216.pdf) come in, which are able to generate sparse embeddings, sometimes called lexical weights. Since `SPLADE` has been widely discussed, here we introduce the generation of dense and sparse vectors by the `BGE-M3` model.

`BGE-M3` is a multi-functionality model, it can simultaneously perform the three common retrieval functionalities of the embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval. Next, let's explore together the sparse embedding generation capability of the `BGE-M3` model.

Let's start with the `BGE-M3` tutorial. We will generate a dense embedding and a sparse weight for tokens:

```python
# pip install -U FlagEmbedding

from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)
sentences = ["What is BM25?", "Definition of BM25"]

output = model.encode(sentences, return_dense=True, return_sparse=True)

dense_embedding = output['dense_vecs']

# you can see the dense embedding for the two texts above:
print(dense_embedding)
# [[-0.02501617 -0.04525582 -0.01972856 ... -0.0099566   0.01677509
#   0.01292699]
# [-0.0106499  -0.04731942 -0.01477693 ... -0.0149255   0.01040097
#   0.00965083]

sparse_weight = model.convert_id_to_token(output['lexical_weights'])

# you can see the weight for each token:
print(sparse_weight)
# [{'What': 0.10430284, 'is': 0.10090457, 'BM': 0.2635918, '25': 0.3382988, '?': 0.052101523}, 
# {'Definition': 0.14298248, 'of': 0.076763615, 'BM': 0.2577639, '25': 0.33806682}]
```

Youâ€™ll notice that we have created a text-to-word matrix for all sentences:

| TEXT               | What | is   | BM   | 25   | ?    | Definition | of   | ...  |
| ------------------ | ---- | ---- | ---- | ---- | ---- | ---------- | ---- | ---- |
| What is BM25?      | 0.08 | 0.08 | 0.13 | 0.25 | 0.04 | 0          | 0    | ...  |
| Definition of BM25 | 0    | 0    | 0.25 | 0.33 | 0    | 0.14       | 0.06 | ...  |


The vectors corresponding to these two sentences show a significant sparsity. If we look at the whole vocabulary, this sparsity is even more pronounced, with most words appearing in only a few sentences, with corresponding weights.

By batch inputting text into sparse embedding models like `BGE-M3`, we can easily obtain sparse embeddings from it.

```python
# pip install -U numpy

import numpy as np

# vocabulary size = 32000 for BGE-M3
# equals to len(vocabs)
vocabs = ["What", "is", "BM", "25", "Definition", "of", ...]
vocab_rev = {v:i for i, v in enumerate(vocabs)}

sparse_embedding = np.zeros(len(vocabs))
index = np.array([vocab_rev[v] for v in sparse_weight.keys()])
value = np.array(list(sparse_weight.values()))
# sparse vector embedding of text
sparse_embedding[index] = value
```

Now we have both the sparse and dense embeddings. It's time to insert them into the vector database.

## Simultaneous Storage of Sparse and Dense vectors

First, a deployed `pgvecto.rs` instance is required for vector storage and retrieval. This can be achieved by running `pgvecto-rs` official docker image:

```bash
docker run \
  --name pgvecto-rs-demo \
  -e POSTGRES_PASSWORD=mysecretpassword \
  -p 5432:5432 \
  -d tensorchord/pgvecto-rs:pg16-v0.2.1
```

Now that the only necessary dependency is installed, let's create a table to store vectors. 

`pgvecto.rs` supports dense or sparse vectors by different data types:

- `vector` for normal dense vector
- `svector` for sparse vector

In this example, we create a table named `documents` with four columns: an ID column(`id`), a text column(`text`) and two vector columns(`sparse` for sparse vector and `dense` for dense vector). 

For the model `BGE-M3`, the dense embedding is defined to have 1024 dimensions. While sparse embedding should be assigned 32000 dimension, equal to LLaMA vocabulary size.

```python
URL = "postgresql://postgres:mysecretpassword@localhost:5432/postgres"

with psycopg.connect(URL) as conn:
    conn.execute("""
        CREATE TABLE documents (
            id SERIAL PRIMARY KEY, 
            text TEXT NOT NULL,
            dense vector(1024) NOT NULL
            sparse svector(32000) NOT NULL);
    """
    )
```

After the table is created, we can now insert embeddings into the table and create indexes for vector columns.

```python
with psycopg.connect(URL) as conn:
    for text, sparse, dense in zip(sentences, sparse_embedding, dense_embedding):
        conn.execute("INSERT INTO documents (text, dense, sparse) VALUES (%s, %s, %s);", (text, emb, dense))
    conn.execute("""
        CREATE INDEX ON documents 
        USING vectors (sparse vector_dot_ops) 
        WITH (options = \"[indexing.hnsw]\");
    """)
    conn.execute("""
        CREATE INDEX ON documents 
        USING vectors (dense vector_l2_ops) 
        WITH (options = \"[indexing.hnsw]\");
    """)
```

For dense vectors, `vector_l2_ops` is the Squared Euclidean distance, which is most commonly used.
$$
D_{L2} = \Sigma (x_i - y_i) ^ 2
$$
For sparse vectors, `vector_dot_ops` is the dot product, which is much more efficient.
$$
D_{dot} = - \Sigma x_iy_i
$$


For more information about index types in `pgvecto.rs`, please [check out our documentation](https://docs.pgvecto.rs/getting-started/overview.html).

## Retrieving and Mixing Results

With the vectors stored in the database, we can now efficiently query them using the `pgvecto.rs` extension. To query the nearest vectors to a given input text, we need to build a function to encode any text with the same model..

It is the same as in the previous chapter:

```python
def embedding(sentence: str):
    """
    Create dense and sparse embeddings from text
    """
    output = model.encode(sentence, return_dense=True, return_sparse=True, return_sparse_embedding=False)
    dense_embedding = output['dense_vecs']
    sparse_weight = model.convert_id_to_token(output['lexical_weights'])
    sparse_embedding = np.zeros(len(vocabs))
    index = np.array([vocab_rev[v] for v in sparse_weight.keys()])
    value = np.array(list(sparse_weight.values()))
    sparse_embedding[index] = value
    return dense_embedding, sparse_embedding
```

To retrieve sparse and dense vectors, `pgvecto.rs` provides a user-friendly `SELECT` statement:

```python
dense_embedding, sparse_embedding = embedding("The text you want to search...")

with psycopg.connect(URL) as conn:
	cur = conn.execute(
            "SELECT text FROM documents ORDER BY sparse <-> %s LIMIT 50;",
            (sparse_embedding,),
        )
    dense_result = cur.fetchall()
    
    cur = conn.execute(
            "SELECT text FROM documents ORDER BY dense <-> %s LIMIT 50;",
            (dense_embedding,),
        )
    sparse_result = cur.fetchall()
    
    # Merge result from sparse and dense search
    mix_text = set([r[0] for r in dense_result]).union([r[0] for r in sparse_result])
```

To rerank the merged result, you can use popular fusion methods like Reciprocal Ranked Fusion (RRF). But to make better use of `BGE-M3`, let's introduce the `Reranker` model of  `BGE-M3`. Unlike the embedding model, `Reranker` uses text input and directly outputs their similarity, or score.

We can initialize a `Reranker` model and feed it with mixed candidate texts. With this method, reranking can be as simple as sorting by the `Reranker` model's estimated score.

```python
from FlagEmbedding import FlagReranker

reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)

# Get relevance scores (higher scores indicate more relevance)
scores = reranker.compute_score([["The text you want to search...", candidate] for candidate in mix_text])
# Rerank text from merged dense and sparse embeddings
reranked_text = [t for _, t in sorted(zip(scores, mix_text))]
```

Congratulations! We have now completed our first step into the world of hybrid vector search.